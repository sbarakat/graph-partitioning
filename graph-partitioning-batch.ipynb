{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from graph_partitioning import GraphPartitioning, utils\n",
    "\n",
    "cols = [\"WASTE\", \"CUT RATIO\", \"EDGES CUT\", \"TOTAL COMM VOLUME\", \"MODULARITY\"]\n",
    "\n",
    "pwd = %pwd\n",
    "\n",
    "config = {\n",
    "\n",
    "    \"DATA_FILENAME\": os.path.join(pwd, \"data\", \"oneshot_fennel_weights.txt\"),\n",
    "    \"OUTPUT_DIRECTORY\": os.path.join(pwd, \"output\"),\n",
    "\n",
    "    # Set which algorithm is run for the PREDICTION MODEL.\n",
    "    # Either: 'FENNEL' or 'SCOTCH'\n",
    "    \"PREDICTION_MODEL_ALGORITHM\": \"FENNEL\",\n",
    "\n",
    "    # Alternativly, read input file for prediction model.\n",
    "    # Set to empty to generate prediction model using algorithm value above.\n",
    "    \"PREDICTION_MODEL\": \"\",\n",
    "\n",
    "    \n",
    "    \"PARTITIONER_ALGORITHM\": \"FENNEL\",\n",
    "\n",
    "    # File containing simulated arrivals. This is used in simulating nodes\n",
    "    # arriving at the shelter. Nodes represented by line number; value of\n",
    "    # 1 represents a node as arrived; value of 0 represents the node as not\n",
    "    # arrived or needing a shelter.\n",
    "    \"SIMULATED_ARRIVAL_FILE\": os.path.join(pwd, \"data\", \"simulated_arrival.txt\"),\n",
    "\n",
    "    # File containing the geographic location of each node.\n",
    "    \"POPULATION_LOCATION_FILE\": os.path.join(pwd, \"data\", \"population_location.csv\"),\n",
    "\n",
    "    # Number of shelters\n",
    "    \"num_partitions\": 4,\n",
    "\n",
    "    # The number of iterations when making prediction model\n",
    "    \"num_iterations\": 10,\n",
    "\n",
    "    # Percentage of prediction model to use before discarding\n",
    "    # When set to 0, prediction model is discarded, useful for one-shot\n",
    "    \"prediction_model_cut_off\": 0.10,\n",
    "\n",
    "    # Alpha value used in one-shot (when restream_batches set to 1)\n",
    "    \"one_shot_alpha\": 0.5,\n",
    "\n",
    "    # Number of arrivals to batch before recalculating alpha and restreaming.\n",
    "    # When set to 1, one-shot is used with alpha value from above\n",
    "    \"restream_batches\": 100,\n",
    "\n",
    "    # When the batch size is reached: if set to True, each node is assigned\n",
    "    # individually as first in first out. If set to False, the entire batch\n",
    "    # is processed and empty before working on the next batch.\n",
    "    \"sliding_window\": False,\n",
    "\n",
    "    # Create virtual nodes based on prediction model\n",
    "    \"use_virtual_nodes\": False,\n",
    "\n",
    "    # Virtual nodes: edge weight\n",
    "    \"virtual_edge_weight\": 1.0,\n",
    "\n",
    "\n",
    "    ####\n",
    "    # GRAPH MODIFICATION FUNCTIONS\n",
    "\n",
    "    # Also enables the edge calculation function.\n",
    "    \"graph_modification_functions\": True,\n",
    "\n",
    "    # If set, the node weight is set to 100 if the node arrives at the shelter,\n",
    "    # otherwise the node is removed from the graph.\n",
    "    \"alter_arrived_node_weight_to_100\": False,\n",
    "\n",
    "    # Uses generalized additive models from R to generate prediction of nodes not\n",
    "    # arrived. This sets the node weight on unarrived nodes the the prediction\n",
    "    # given by a GAM.\n",
    "    # Needs POPULATION_LOCATION_FILE to be set.\n",
    "    \"alter_node_weight_to_gam_prediction\": False,\n",
    "\n",
    "    # The value of 'k' used in the GAM will be the number of nodes arrived until\n",
    "    # it reaches this max value.\n",
    "    \"gam_k_value\": 100,\n",
    "\n",
    "    # Alter the edge weight for nodes that haven't arrived. This is a way to\n",
    "    # de-emphasise the prediction model for the unknown nodes.\n",
    "    \"prediction_model_emphasis\": 1.0,\n",
    "    \n",
    "    \n",
    "    # Path to the scotch shared library\n",
    "    \"SCOTCH_LIB_PATH\": \"/usr/local/lib/libscotch.so\",\n",
    "\n",
    "    \"SCOTCH_PYLIB_REL_PATH\": os.path.join(pwd, \"..\", \"csap-graphpartitioning\", \"src\", \"python\")\n",
    "}\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing...\n",
      "........\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "test_dir = os.path.join(pwd, \"data\", \"predition_model_tests\")\n",
    "\n",
    "print(\"\\nProcessing...\")\n",
    "for i in list(range(12,20)): #list(range(1,1001)):\n",
    "    network_file = \"network_{}.txt\".format(i)\n",
    "    arrival_file = \"arrival_0_{}.txt\".format(i)\n",
    "    coordinates_file = \"coordinates_{}.txt\".format(i)\n",
    "\n",
    "    print(\".\", end='', flush=True)\n",
    "    #print(\"\\nProcessing...\",\n",
    "    #      \"\\n\\tnetwork file:\", network_file,\n",
    "    #      \"\\n\\tarrival file:\", arrival_file,\n",
    "    #      \"\\n\\tcoordinates file:\", coordinates_file)\n",
    "\n",
    "    config[\"DATA_FILENAME\"] = os.path.join(test_dir,\n",
    "                                           \"network\",\n",
    "                                           network_file)\n",
    "\n",
    "    config[\"SIMULATED_ARRIVAL_FILE\"] = os.path.join(test_dir,\n",
    "                                                    \"dataset_1_shift_rotate\",\n",
    "                                                    \"simulated_arrival_list\",\n",
    "                                                    \"percentage_of_prediction_correct_0\",\n",
    "                                                    arrival_file)\n",
    "\n",
    "    config[\"POPULATION_LOCATION_FILE\"] = os.path.join(test_dir,\n",
    "                                                      \"coordinates\",\n",
    "                                                      coordinates_file)\n",
    "\n",
    "    gp = GraphPartitioning(config)\n",
    "    gp._quiet = True\n",
    "    gp.load_network()\n",
    "    gp.init_partitioner()\n",
    "    gp.prediction_model()\n",
    "    gp.assign_cut_off()\n",
    "\n",
    "    # run simulation\n",
    "    gp.batch_arrival()\n",
    "\n",
    "    gp.get_metrics()\n",
    "\n",
    "print(\"\\nComplete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wavg(group, avg_name, weight_name):\n",
    "    \"\"\"\n",
    "    Weighted average\n",
    "    \"\"\"\n",
    "    d = group[avg_name]\n",
    "    w = group[weight_name]\n",
    "    try:\n",
    "        return (d * w).sum() / w.sum()\n",
    "    except ZeroDivisionError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined metrics saved to /home/sami/repos/smbwebs/graph-partitioning/output/metrics-combined.csv\n"
     ]
    }
   ],
   "source": [
    "metrics = pd.read_csv(os.path.join(config['OUTPUT_DIRECTORY'], 'metrics.csv'))\n",
    "nonover = pd.read_csv(os.path.join(config['OUTPUT_DIRECTORY'], 'metrics-partitions-nonoverlapping.csv'))\n",
    "over = pd.read_csv(os.path.join(config['OUTPUT_DIRECTORY'], 'metrics-partitions-overlapping.csv'))\n",
    "\n",
    "metrics.set_index(['file'], inplace=True)\n",
    "\n",
    "# get weighted averages.\n",
    "# use population of the partition as the weight\n",
    "nonover_wavg = nonover.groupby(\"file\").apply(wavg, \"network_permanence\", \"population\").to_frame()\n",
    "nonover_wavg.columns = ['network_permanence (wavg)']\n",
    "\n",
    "data = {\n",
    "    'Q (wavg)': over.groupby(\"file\").apply(wavg, \"Q\", \"population\"),\n",
    "    'NQ (wavg)': over.groupby(\"file\").apply(wavg, \"NQ\", \"population\"),\n",
    "    'Qds (wavg)': over.groupby(\"file\").apply(wavg, \"Qds\", \"population\"),\n",
    "    'intraEdges (wavg)': over.groupby(\"file\").apply(wavg, \"intraEdges\", \"population\"),\n",
    "    'interEdges (wavg)': over.groupby(\"file\").apply(wavg, \"interEdges\", \"population\"),\n",
    "    'intraDensity (wavg)': over.groupby(\"file\").apply(wavg, \"intraDensity\", \"population\"),\n",
    "    'modularity degree (wavg)': over.groupby(\"file\").apply(wavg, \"modularity degree\", \"population\"),\n",
    "    'conductance (wavg)': over.groupby(\"file\").apply(wavg, \"conductance\", \"population\"),\n",
    "    'expansion (wavg)': over.groupby(\"file\").apply(wavg, \"expansion\", \"population\"),\n",
    "    'contraction (wavg)': over.groupby(\"file\").apply(wavg, \"contraction\", \"population\"),\n",
    "    'fitness (wavg)': over.groupby(\"file\").apply(wavg, \"fitness\", \"population\"),\n",
    "    'QovL (wavg)': over.groupby(\"file\").apply(wavg, \"QovL\", \"population\")\n",
    "}\n",
    "over_wavg = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# compile a single dataframe with all metrics and write to csv file\n",
    "cols = ['num_partitions', 'num_iterations', 'prediction_model_cut_off',\n",
    "        'one_shot_alpha', 'restream_batches', 'use_virtual_nodes',\n",
    "        'virtual_edge_weight', 'edges_cut', 'waste',\n",
    "        'cut_ratio', 'total_communication_volume',\n",
    "        'network_permanence', 'network_permanence (wavg)',\n",
    "        'Q', 'Q (wavg)',\n",
    "        'NQ', 'NQ (wavg)', \n",
    "        'Qds', 'Qds (wavg)',\n",
    "        'intraEdges', 'intraEdges (wavg)',\n",
    "        'interEdges', 'interEdges (wavg)',\n",
    "        'intraDensity', 'intraDensity (wavg)',\n",
    "        'modularity degree', 'modularity degree (wavg)',\n",
    "        'conductance', 'conductance (wavg)',\n",
    "        'expansion', 'expansion (wavg)',\n",
    "        'contraction', 'contraction (wavg)',\n",
    "        'fitness', 'fitness (wavg)',\n",
    "        'QovL', 'QovL (wavg)'\n",
    "       ]\n",
    "\n",
    "combined_metrics = metrics.join(nonover_wavg).join(over_wavg)[cols]\n",
    "combined_csv = os.path.join(config['OUTPUT_DIRECTORY'], 'metrics-combined.csv')\n",
    "combined_metrics.to_csv(combined_csv)\n",
    "print(\"Combined metrics saved to\", combined_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
